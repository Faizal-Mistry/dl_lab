import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers
from sklearn.metrics import classification_report, confusion_matrix

# Paths and parameters
DATA_DIR = "caltech-101-img"
WEIGHTS_FILE = "vgg16_weights_tf_dim_ordering_tf_kernels_notop (1).h5"
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 10
LR = 0.001

# Load dataset
train_ds = keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="int",
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    validation_split=0.2,
    subset="training",
    seed=42
)
val_ds = keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    labels="inferred",
    label_mode="int",
    image_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    validation_split=0.2,
    subset="validation",
    seed=42
)

class_names = train_ds.class_names
num_classes = len(class_names)
print("Detected classes:", num_classes, class_names)

# Preprocessing function
preprocess = keras.applications.vgg16.preprocess_input
def preprocess_ds(ds):
    return ds.map(lambda x, y: (preprocess(tf.cast(x, tf.float32)), y),
                  num_parallel_calls=tf.data.AUTOTUNE)

train_ds = preprocess_ds(train_ds).cache().prefetch(tf.data.AUTOTUNE)
val_ds = preprocess_ds(val_ds).cache().prefetch(tf.data.AUTOTUNE)

# Build VGG16 transfer learning model
from tensorflow.keras.applications import VGG16

base_model = VGG16(include_top=False, weights=None,
                   input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))
base_model.load_weights(WEIGHTS_FILE)
base_model.trainable = False  # Freeze base model

inputs = keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256, activation="relu")(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)
model = keras.Model(inputs, outputs, name="vgg16_transfer")

model.summary()

# Compile
optimizer = optimizers.SGD(learning_rate=LR, momentum=0.9)
model.compile(optimizer=optimizer,
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

# Train
history = model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, verbose=2)

# Validation predictions
y_true = []
y_pred = []
images_for_display = []
labels_for_display = []

for batch_imgs, batch_labels in val_ds.unbatch().batch(BATCH_SIZE):
    preds = model.predict(batch_imgs, verbose=0)
    y_true.extend(batch_labels.numpy().tolist())
    y_pred.extend(np.argmax(preds, axis=1).tolist())
    
    if len(images_for_display) < 16:
        need = 16 - len(images_for_display)
        imgs_np = batch_imgs.numpy()[:need]
        labs_np = batch_labels.numpy()[:need]
        for im_arr, lab in zip(imgs_np, labs_np):
            images_for_display.append(im_arr.copy())
            labels_for_display.append(int(lab))

y_true = np.array(y_true)
y_pred = np.array(y_pred)

# Classification report and confusion matrix
print("\nValidation classification report:")
print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))
cm = confusion_matrix(y_true, y_pred)
print("Confusion matrix shape:", cm.shape)

# Plot training history
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history["loss"], label="train loss")
plt.plot(history.history["val_loss"], label="val loss")
plt.title("Loss")
plt.xlabel("Epoch")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history["accuracy"], label="train acc")
plt.plot(history.history["val_accuracy"], label="val acc")
plt.title("Accuracy")
plt.xlabel("Epoch")
plt.legend()
plt.tight_layout()
plt.show()

# Image display helper
def deprocess_vgg(x):
    y = x.copy()
    y[..., 0] += 103.939
    y[..., 1] += 116.779
    y[..., 2] += 123.68
    y = y[..., ::-1]
    y = np.clip(y, 0, 255).astype("uint8")
    return y

# Display sample predictions
plt.figure(figsize=(12,12))
for i, img in enumerate(images_for_display[:16]):
    plt.subplot(4,4,i+1)
    img_np = deprocess_vgg(img)
    preds = model.predict(np.expand_dims(img, axis=0), verbose=0)
    pred_label = class_names[int(np.argmax(preds))]
    true_label = class_names[int(labels_for_display[i])]
    plt.imshow(img_np)
    plt.title(f"P:{pred_label}\nT:{true_label}")
    plt.axis("off")
plt.tight_layout()
plt.show()



 1. Deep Learning and CNN

â—â€‹ Deep Learning is a subset of Machine Learning that uses neural networks with
multiple layers to automatically learn features from data.â€‹
â—â€‹ Convolutional Neural Networks (CNNs) are used for image-related tasks such as
classification, detection, and segmentation.â€‹
CNNs use convolutional layers to extract spatial features (edges, textures, shapes)
from images.â€‹

ðŸ”¹ 2. Transfer Learning
â—â€‹ Transfer Learning is a method where a pre-trained model (trained on a large
dataset like ImageNet) is reused for a new but related task.â€‹
â—â€‹ Instead of training a model from scratch, we use the feature extraction layers from
a pre-trained network and only train the final layers on our dataset.â€‹
â—â€‹ This helps in:â€‹
â—‹â€‹ Faster trainingâ€‹
â—‹â€‹ Better accuracy with limited dataâ€‹
â—‹â€‹ Avoiding overfittingâ€‹

ðŸ”¹ 3. VGG16 Architecture
â—â€‹ VGG16 is a popular CNN model developed by Oxfordâ€™s Visual Geometry Group
(VGG).â€‹
â—â€‹ It consists of:â€‹
â—‹â€‹ 13 convolutional layersâ€‹
â—‹â€‹ 5 max pooling layersâ€‹
â—‹â€‹ 3 fully connected layersâ€‹
â—â€‹ It uses 3Ã—3 convolution filters and ReLU activation.â€‹
â—â€‹ The model is trained on ImageNet dataset (over 1 million images, 1000 classes).â€‹

ðŸ”¹ 4. Dataset â€“ Caltech-101
â—â€‹ The Caltech-101 dataset contains 101 object categories (e.g., animals, vehicles,
instruments, etc.).â€‹
â—â€‹ Each class has 40â€“800 images.â€‹
â—â€‹ The images are of varying sizes and are resized to 224Ã—224 pixels for VGG16 input.â€‹

ðŸ”¹ 5. Model Workflow
1.â€‹ Dataset Preparation:â€‹
â—‹â€‹ Images are loaded from folders using
image_dataset_from_directory().â€‹
â—‹â€‹ Split into training (80%) and validation (20%).â€‹
â—‹â€‹ Labels are automatically inferred from folder names.â€‹
â—‹â€‹ Preprocessing is done using vgg16.preprocess_input() to match
VGG16â€™s input format.â€‹
2.â€‹ Model Building:â€‹
â—‹â€‹ Import VGG16 with include_top=False to exclude its final classification
layer.â€‹
â—‹â€‹ Load pre-trained ImageNet weights.â€‹
â—‹â€‹ Freeze base layers to keep pre-learned features.â€‹
â—‹â€‹ Add new layers:â€‹
â– â€‹ GlobalAveragePooling2D â€“ reduces spatial dimensions.â€‹
â– â€‹ Dense(256, ReLU) â€“ fully connected layer.â€‹
â– â€‹ Dropout(0.5) â€“ prevents overfitting.â€‹
â– â€‹ Dense(num_classes, Softmax) â€“ output layer for classification.â€‹

3.â€‹ Compilation:â€‹
â—‹â€‹ Optimizer: SGD (Stochastic Gradient Descent) with momentum = 0.9.â€‹
â—‹â€‹ Loss: Sparse Categorical Crossentropy.â€‹
â—‹â€‹ Metric: Accuracy.â€‹
4.â€‹ Training:â€‹
â—‹â€‹ Train for 10 epochs on training data.â€‹
â—‹â€‹ Validate on validation data.â€‹
5.â€‹ Evaluation:â€‹
â—‹â€‹ Compute classification report and confusion matrix.â€‹
â—‹â€‹ Plot loss and accuracy curves.â€‹
6.â€‹ Visualization:â€‹
â—‹â€‹ Display sample images with predicted and true labels.â€‹

ðŸ”¹ 6. Important Concepts
Concept

Description

Feature Extraction

Using pre-trained CNN layers to extract meaningful features from
images.

Freezing Layers

Prevents updating weights of pre-trained layers during training.

Fine-Tuning

Optionally unfreezing top layers of the base model for slight
adjustment.

Global Average
Pooling

Converts feature maps into a single vector per feature channel.

Dropout

Randomly drops neurons during training to reduce overfitting.

ðŸ”¹ 7. Advantages of Transfer Learning
â—â€‹ Reduces training time.â€‹

â—â€‹ Requires less data.â€‹
â—â€‹ Improves accuracy for smaller datasets.â€‹
â—â€‹ Leverages previously learned general features.â€‹

ðŸ§® Result:
The VGG16 Transfer Learning model successfully classifies images from the Caltech-101
dataset with good accuracy.â€‹
Training and validation graphs show the modelâ€™s performance improvement over epochs.

ðŸ“Š Conclusion:
Transfer Learning using VGG16 effectively performs image classification with limited data by
reusing learned features from ImageNet. The model achieves high validation accuracy and
demonstrates the efficiency of pre-trained CNN architectures.


